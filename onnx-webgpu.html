<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>onnx</title>
</head>

<body>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <h1>onnx webgpu</h1>
</body>
<script type="module">
    async function main() {
        try {
            // create a new session and load the specific model.
            // const session = await ort.InferenceSession.create('data/sims.onnx');
            // Fails with e = 42912032??? Maybe model isn't compatible with webgpu?
            const session = await ort.InferenceSession.create('http://localhost:3000/models/default.onnx', { executionProviders: ['webgpu'] });

            // Works: From https://github.com/akbartus/DepthAnything-on-Browser/blob/main/interactive-v2/webgpu-sliders.html
            // const myOrtSession = await ort.InferenceSession.create("https://cdn.glitch.me/0f5359e2-6022-421b-88f7-13e276d0fb33/depthanythingv2-vits-dynamic-quant.onnx", { executionProviders: ['webgpu'] });

            console.log('sims model loaded');

            // prepare inputs. a tensor need its corresponding TypedArray as data
            const batch_size = 8;
            const X = new ort.Tensor('float32', new Float32Array(batch_size * 33694), [batch_size, 33694]);
            for (let i = 0; i < 100; i++) {
                X.data[i] = 0.5;
            }

            // feed inputs and run
            const results = await session.run({ "input.1": X });
            console.log('inference done');
            console.log(results);
            document.write(`<div>Cell 0 Predictions: ${results["826"].cpuData.slice(0, 8)}</div>`);
        } catch (e) {
            document.write(`failed to inference ONNX model: ${e}.`);
        }
    }
    main();
    console.log('done');
</script>

</html>